---
layout: post
title: 'Pytorch便签'
date: 2022-2-11
author: 不显电性
cover: 'http://commcheck396.github.io/assets/img/2022_1_26/pytorch.jpg'
tags: ML Python Pytorch
---

# ~~不能再拖了~~

> Pythorch，江湖人称小Numpy

[Pytorch常见运算](https://pytorch.org/docs/stable/torch.html)

<br/>

## Mostly Used

Numpy向Torch的转化
`torch_data = torch.from_numpy(np_data)`

<br/>

Torch的矩阵乘法
```python

torch.mm(tensor, tensor)
# 'mm' stands for 'matrix multiply'
# 注意此处与Numpy的区别，不能直接利用.dot()进行运算 

```

<br/>  

Troch的数学运算规则几乎与Numpy一致，[请参考](https://pytorch.org/docs/stable/torch.html)

<br/>

Torch中，若要进行反向传播，需要利用variable进行运算，variable可以一次性将所有修改幅度 (梯度) 都计算出来, 而tensor就没有这个能力。  
但variable中的数据需要tensor类型导入：
```python 
tensor = torch.FloatTensor([[1,2],[3,4]])
# requires_grad是参不参与误差反向传播, 要不要计算梯度
variable = Variable(tensor, requires_grad=True)
```

升维
```python
x=torch.unsqueeze(x,dim=1)
```

## 激励函数

Torch 中的激励函数有很多, 不过我们平时要用到的就这几个： **`relu`**, `sigmoid`, `tanh`, `softplus`。 
如何利用激励函数:
```python
# following are popular activation functions
y_relu = torch.relu(x).numpy()
y_sigmoid = torch.sigmoid(x).numpy()
y_tanh = torch.tanh(x).numpy()
y_softplus = F.softplus(x).numpy() # there's no softplus in torch
```

## 神经网络的搭建与训练

### 普通搭建方法：  

```python
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output): # 分别代表输入，该层神经元个数，输出
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.predict = torch.nn.Linear(n_hidden, n_output)

    def forward(self, x):
        x = F.relu(self.hidden(x)) # 利用relu激励函数
        x = self.predict(x)
        return x

net = Net(1, 10, 1) # 搭建含有一层十个hidden神经元的神经网络

# 结构：
# Net (
#   (hidden): Linear (1 -> 10)
#   (predict): Linear (10 -> 1)
# )

```

~~🐶都不用，有轮子还不用？~~  
 <br/>

### ~~我tm直接~~快速搭建
```python
net = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(), # 注意要大写，这里的ReLU是一个class
    torch.nn.Linear(10, 1)
)

# 结构：
# Sequential (
#   (0): Linear (1 -> 10)
#   (1): ReLU ()
#   (2): Linear (10 -> 1)
# )
```

### 训练方法  

#### 回归拟合
```python
# optimizer 是训练的工具
optimizer = torch.optim.SGD(net.parameters(), lr=0.2)  # 传入 net 的所有参数, learning rate
loss_func = torch.nn.MSELoss()      # 预测值和真实值的误差计算公式 (均方差)

for t in range(100):
    prediction = net(x)     # 喂给 net 训练数据 x, 输出预测值
    loss = loss_func(prediction, y)     # 计算两者的误差,prediction要在前
    optimizer.zero_grad()   # 清空上一步的残余更新参数值
    loss.backward()         # 误差反向传播, 计算参数更新值
    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上
```

#### 区分类型
```python
net = Net(n_feature=2, n_hidden=10, n_output=2) # 几个类别就几个 output

# optimizer 是训练的工具
optimizer = torch.optim.SGD(net.parameters(), lr=0.02)  # 传入 net 的所有参数,learning rate
loss_func = torch.nn.CrossEntropyLoss() 
# 注意loss function的改变，此时的输出是二维数据，比如[0.2,0.8]，代表该点属于第一类别的概率为0.2，属于第二类别的概率为0.8，所以不能利用与regression相同的loss function，需要用这个


for t in range(100):
    out = net(x)     # 喂给 net 训练数据 x, 输出分析值
    loss = loss_func(out, y)     # 计算两者的误差
    optimizer.zero_grad()   # 清空上一步的残余更新参数值
    loss.backward()         # 误差反向传播, 计算参数更新值
    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上

```

## 神经网络的保存与提取

### save
```python
torch.save(net1, 'net.pkl')  # 保存整个网络
torch.save(net1.state_dict(), 'net_params.pkl')   # 只保存网络中的参数 (速度快, 占内存少)
```

### load
```python
# 提取整个网络

net2 = torch.load('net.pkl') # 提取网络
prediction = net2(x) # 使用网络


# 提取网络参数

# 需要新建 net3，再将提取的参数导入该网络中
net3 = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)

net3.load_state_dict(torch.load('net_params.pkl')) # 将保存的参数复制到 net3
prediction = net3(x) # 使用网络
 
```

## Optimizer

~~无脑Adam就可以了~~
几种常见的优化器：`SGD`, `Momentum`, `RMSprop`, `Adam`  
使用方法：
```python
# different optimizers
opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)
opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8) # 注意momentum就是SGD的套壳
opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)
opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))
# 首选后两个Optimizer，效果较佳
```